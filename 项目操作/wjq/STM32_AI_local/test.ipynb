{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557eb1a8-8339-4ee6-b6a2-46fdde58ab4a",
   "metadata": {},
   "source": [
    "###  量化测量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce947caf-3024-413b-b9cc-a6794a3f127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入库\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b206a35-512b-4d47-b963-de9f3f51435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "  \n",
    "# 添加模块的父目录到sys.path  \n",
    "sys.path.append('./tiny_training/tinytraining/algorithm/quantize')  \n",
    "  \n",
    "# 现在你可以导入模块中的函数和类了  \n",
    "from quantized_ops import to_pt, QuantizedAvgPool, QuantizedConv2d, QuantizedElementwise, QuantizedMbBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5111940-d130-45ab-9584-4992e0bc44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTIZED_GRADIENT = False\n",
    "ROUNDING = 'round'\n",
    "CONV_W_GRAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff0a883-1bb3-4504-9ab5-507301b7792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "if QUANTIZED_GRADIENT:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be26db79-6847-44bf-b0b5-5700abb1d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_tensor(x):\n",
    "    if ROUNDING == 'round':\n",
    "        return x.round()\n",
    "    elif ROUNDING == 'floor':\n",
    "        return x.int().float()\n",
    "    elif ROUNDING == 'debug':\n",
    "        return x\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a688730f-9d10-495b-9fa2-9bd17c161524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _QuantizedAvgPoolFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.input_shape = x.shape\n",
    "        assert x.dtype == torch.float32\n",
    "        x = x.mean([-1, -2], keepdim=True)\n",
    "        return round_tensor(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_shape = ctx.input_shape\n",
    "        grad_input = grad_output.repeat(1, 1, *input_shape[-2:]) / (input_shape[-1] * input_shape[-2])\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b0b678-a5b0-4ab1-be5a-204f5f9e2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedAvgPoolDiff(QuantizedAvgPool):\n",
    "    def forward(self, x):\n",
    "        x = _QuantizedAvgPoolFunc.apply(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bb62482-8b83-4058-9980-4c9b6cc997c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _QuantizedElementwiseAddFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x1, x2, zero_x1, zero_x2, zero_y, scale_x1, scale_x2, scale_y):\n",
    "        # ensure x1 and x2 are int\n",
    "        x1 = x1.round()\n",
    "        x2 = x2.round()\n",
    "        assert x1.shape == x2.shape\n",
    "        ctx.save_for_backward(scale_x1, scale_x2, scale_y)\n",
    "\n",
    "        x1 = (x1 - zero_x1) * scale_x1\n",
    "        x2 = (x2 - zero_x2) * scale_x2\n",
    "\n",
    "        out = x1 + x2\n",
    "        out = round_tensor(out / scale_y)\n",
    "        out = out + zero_y\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # only return gradient of zero_y, zero_x1, zero_x2, x1, x2\n",
    "        scale_x1, scale_x2, scale_y = ctx.saved_tensors\n",
    "\n",
    "        grad_zero_y = grad_output.sum([0, 2, 3])\n",
    "        grad_sum = grad_output / scale_y.item()\n",
    "        grad_x1 = grad_sum * scale_x1.item()\n",
    "        grad_x2 = grad_sum * scale_x2.item()\n",
    "        grad_zero_x1 = - grad_x1.sum([0, 2, 3])\n",
    "        grad_zero_x2 = - grad_x2.sum([0, 2, 3])\n",
    "        return grad_x1, grad_x2, grad_zero_x1, grad_zero_x2, grad_zero_y, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "166c71ee-b03b-4177-861a-6a64b1f01db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedElementwiseDiff(QuantizedElementwise):\n",
    "    def __init__(self, operator, zero_x1, zero_x2, zero_y, scale_x1, scale_x2, scale_y):\n",
    "        super().__init__(operator, zero_x1, zero_x2, zero_y, scale_x1, scale_x2, scale_y)\n",
    "        assert self.operator == 'add'  # for mult, we do not support bias-only update\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return _QuantizedElementwiseAddFunc.apply(x1, x2,\n",
    "                                                  self.zero_x1, self.zero_x2, self.zero_y,\n",
    "                                                  self.scale_x1, self.scale_x2, self.scale_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd40985b-ed90-4b79-a239-3939e5581a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TruncateActivationRange(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a_bit):\n",
    "        ctx.a_bit = a_bit\n",
    "        binary_mask = (- 2 ** (a_bit - 1) <= x) & (x <= 2 ** (a_bit - 1) - 1)\n",
    "        ctx.save_for_backward(binary_mask)\n",
    "        return x.clamp(- 2 ** (a_bit - 1), 2 ** (a_bit - 1) - 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        binary_mask, = ctx.saved_tensors\n",
    "        grad_x = grad_output * binary_mask\n",
    "        return grad_x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa3f14f9-2c8d-420a-8f92-f068c47f5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _QuantizedConv2dFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias, zero_x, zero_y, effective_scale, stride, padding, dilation, groups):\n",
    "        x = x.round()  # ensure x is int\n",
    "        weight = weight.round()  # ensure weight is int\n",
    "\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        ctx.dilation = dilation\n",
    "        ctx.groups = groups\n",
    "        ctx.input_size = x.shape\n",
    "        ctx.weight_size = weight.shape\n",
    "\n",
    "        # weight = weight.int()  # - self.zero_w\n",
    "        x = x - zero_x\n",
    "\n",
    "        if CONV_W_GRAD:\n",
    "            ctx.save_for_backward(weight, effective_scale, x)\n",
    "        else:\n",
    "            ctx.save_for_backward(weight, effective_scale)\n",
    "\n",
    "        out = F.conv2d(x, weight, None, stride, padding, dilation, groups)\n",
    "        out = round_tensor(out)  # ensure output is still int\n",
    "        # here we allow bias saved as fp32, and round to int during inference (keep fp32 copy in memory)\n",
    "        out = out + bias.view(1, -1, 1, 1)  # Confirmed: we don't need to cast bias\n",
    "        out = round_tensor(out * effective_scale.view(1, -1, 1, 1))\n",
    "        out = out + zero_y\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # effective_scale = scale_x * scale_w / scale_y\n",
    "        # b_quantized = b / (w_scales * x_scale), so we may wanna compute grad_b / (w_scale * x_scale)\n",
    "        # which is grad_b / (effective_scale * scale_y)\n",
    "        if CONV_W_GRAD:\n",
    "            weight, effective_scale, _x = ctx.saved_tensors\n",
    "        else:\n",
    "            weight, effective_scale = ctx.saved_tensors\n",
    "\n",
    "        grad_zero_y = grad_output.sum([0, 2, 3])\n",
    "        _grad_conv_out = grad_output * effective_scale.view(1, -1, 1, 1)\n",
    "        grad_bias = _grad_conv_out.sum([0, 2, 3])\n",
    "        _grad_conv_in = torch.nn.grad.conv2d_input(ctx.input_size, weight, _grad_conv_out,\n",
    "                                                   stride=ctx.stride, padding=ctx.padding,\n",
    "                                                   dilation=ctx.dilation, groups=ctx.groups)\n",
    "        grad_zero_x = - _grad_conv_in.sum([0, 2, 3])\n",
    "        grad_x = _grad_conv_in\n",
    "\n",
    "        if CONV_W_GRAD:\n",
    "            grad_w = torch.nn.grad.conv2d_weight(_x, ctx.weight_size, _grad_conv_out,\n",
    "                                                 stride=ctx.stride, padding=ctx.padding,\n",
    "                                                 dilation=ctx.dilation, groups=ctx.groups)\n",
    "        else:\n",
    "            grad_w = None\n",
    "\n",
    "        from core.utils.config import configs\n",
    "        if configs.backward_config.quantize_gradient:  # perform per-channel quantization\n",
    "            # quantize grad_x and grad_w\n",
    "            from .quantize_helper import get_weight_scales\n",
    "            w_scales = get_weight_scales(grad_w, n_bit=8)\n",
    "            grad_w = (grad_w / w_scales.view(-1, 1, 1, 1)).round() * w_scales.view(-1, 1, 1, 1)\n",
    "            x_scales = get_weight_scales(grad_x.transpose(0, 1))\n",
    "            grad_x = (grad_x / x_scales.view(1, -1, 1, 1)).round() * x_scales.view(1, -1, 1, 1)\n",
    "\n",
    "        return grad_x, grad_w, grad_bias, grad_zero_x, grad_zero_y, None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcb0e8cb-b056-467a-b6ed-ac6c80cac525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedConv2dDiff(QuantizedConv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros',\n",
    "                 zero_x=0, zero_w=0, zero_y=0,  # keep same args\n",
    "                 effective_scale=None,\n",
    "                 w_bit=8, a_bit=None,\n",
    "                 ):\n",
    "        super(QuantizedConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                                              padding, dilation, groups, bias, padding_mode)\n",
    "        self.register_buffer('zero_x', to_pt(zero_x))\n",
    "        # self.register_buffer('zero_w', to_pt(zero_w))\n",
    "        self.register_buffer('zero_y', to_pt(zero_y))\n",
    "        from ..core.utils.config import configs\n",
    "        if configs.backward_config.train_scale:\n",
    "            print('Note: the scale is also trained...')\n",
    "            self.register_parameter('effective_scale', torch.nn.Parameter(effective_scale))\n",
    "        else:\n",
    "            self.register_buffer('effective_scale', effective_scale)\n",
    "\n",
    "        self.w_bit = w_bit\n",
    "        self.a_bit = a_bit if a_bit is not None else w_bit\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = _QuantizedConv2dFunc.apply(x, self.weight, self.bias, self.zero_x, self.zero_y, self.effective_scale,\n",
    "                                         self.stride, self.padding, self.dilation, self.groups)\n",
    "        return _TruncateActivationRange.apply(out, self.a_bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39c730a2-03fc-4a73-a7ca-47df4dd2ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMbBlockDiff(QuantizedMbBlock):\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        if self.q_add is not None:\n",
    "            if self.residual_conv is not None:\n",
    "                x = self.residual_conv(x)\n",
    "            out = self.q_add(x, out)\n",
    "            return _TruncateActivationRange.apply(out, self.a_bit)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af0a99f2-8a71-40da-9b6b-2626ff8700fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledLinear(torch.nn.Linear):\n",
    "    # a fp version of fc used for training\n",
    "    def __init__(self, in_features: int, out_features: int, scale_x, zero_x, bias: bool = True,\n",
    "                 device=None, dtype=None, norm_feat=False):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.register_buffer('scale_x', to_pt(scale_x))\n",
    "        self.register_buffer('zero_x', to_pt(zero_x))\n",
    "\n",
    "        self.norm_feat = norm_feat\n",
    "        if norm_feat:\n",
    "            self.bias.data.fill_(2.)\n",
    "            self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.squeeze(-1).squeeze(-1) - self.zero_x.detach().view(1, -1)) * self.scale_x.detach().view(1, -1)\n",
    "        if self.norm_feat:\n",
    "            x_norm = x.div(torch.norm(x, p=2, dim=1).view(-1, 1) + self.eps)\n",
    "            weight_norm = self.weight.div(torch.norm(self.weight, p=2, dim=1).view(-1, 1) + self.eps)\n",
    "            cos_dist = (x_norm @ weight_norm.T) * self.bias.view(1, -1)\n",
    "            return cos_dist\n",
    "        else:\n",
    "            return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63237fa-6ea4-4a5c-a75e-7ca91bc58e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
